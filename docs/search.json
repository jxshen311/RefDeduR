[{"path":"/articles/RefDeduR.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"scientific literature grows exponentially research becomes increasingly interdisciplinary, accurate high-throughput reference deduplication vital evidence synthesis studies (e.g., systematic reviews, meta-analyses) ensure completeness datasets reduce manual screening burden. address emerging needs, developed RefDeduR. modularize deduplication pipeline finely-tuned text normalization, three-step exact matching, two-step fuzzy matching processes. package features decision-tree algorithm considers preprints conference proceedings co-exist peer-reviewed version. , demonstrate functionality RefDeduR example pipeline.","code":""},{"path":"/articles/RefDeduR.html","id":"example-dataset","dir":"Articles","previous_headings":"","what":"Example dataset","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"use example dataset demonstrate recommended pipeline RefDeduR. dataset contains bibliographic records (n = 6384) retrieved systematic review indoor surface microbiome studies. systematic search conducted 2022-01-10 3 platforms (.e., PubMed, Web Science, Scopus).","code":""},{"path":"/articles/RefDeduR.html","id":"pre-processing-transliterate-non-ascii-characters","dir":"Articles","previous_headings":"","what":"Pre-processing: transliterate non-ASCII characters","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"transliteration process includes 2 parts: (1) transliterate common Greek letters names (e.g., α alpha, β beta) (2) transliterate accented characters ASCII characters (e.g., á , ä ). Rationale: increases chance successful deduplication exact matching. also reduces noises partitioning dataset first 2 letters first_author_last_name_norm fuzzy matching step. example, record titled “Carriage population genetics extended spectrum β-lactamase-producing Escherichia coli cats dogs New Zealand” sometimes title “Carriage population genetics extended spectrum beta-lactamase-producing Escherichia coli cats dogs New Zealand”. Author names “Álvarez-Fraga, L. Pérez, .” sometimes “Alvarez-Fraga, L. Perez, .”. ⚒️ Alternatively, python scripts developed basis unidecode package provided. scripts ready run terminal. Performance R function python scripts generally similar, little difference induced difference R package stringi python package unidecode.","code":"# Get the path to the example dataset input_file <- system.file(\"extdata\", \"dataset_raw.bib\", package = \"RefDeduR\")  # Specify the path to the output file. Here we put it in the same directory but you can modify the path to wherever you want to store the output file. transliterated_file <- system.file(\"extdata\", \"dataset_transliterated.bib\", package = \"RefDeduR\")  norm_transliteration(input_file, transliterated_file, method = c(\"greek_letter-name\", \"any-ascii\")) # Transliterate common Greek letters to their names python transliteration_greek_to_name.py <path/to/input_file> <path/to/output_file>  # Transliterate accented characters to ASCII characters  python transliteration_unaccent.py <path/to/input_file> <path/to/output_file>"},{"path":"/articles/RefDeduR.html","id":"read-the-bibliographic-file-into-a-data-frame","dir":"Articles","previous_headings":"","what":"Read the bibliographic file into a data frame","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"use function revtools::read_bibliography() read transliterated bibTex file data frame. recommend using bibTex files . According past experience, reading .ris file seems result formatting errors. Alternative function: synthesisr::read_refs() 🗒️ Comparison two import functions: synthesisr::read_refs() seems better parsing special characters. “β-α-β” can retained, text becomes “Î²-Î±-Î²” using revtools::read_bibliography(). However, potential benefit revtools::read_bibliography() keeps citation key (e.g., “RN13774” first row record “@article{RN13774,”) column named “label”. .bib file exported Endnote (case example dataset), citation key can serve unique identifier. information also preserved Covidence export. Covidence online systematic review management platform, typical downstream step following reference deduplication. preserving citation key (unique identifier) across processes desired, consider switching revtools::read_bibliography() importing twice functions combine data frames. use revtools::read_bibliography() transliterated Greek letters want unique identifier.","code":"# Read the transliterated bibTex file into a data frame b <- revtools::read_bibliography(transliterated_file)  # 6384 rows  # We can check the number of missing values in each column. # Pay attention to `title` column as we expect all records to have titles. # If your dataset has only a few NAs in title, maybe it is worth resolving the missing values manually. If your dataset has a substantial number of NAs in title (according to our experience, this is extremely rare), consider sub-setting the dataset and deduplicating separately. colSums(is.na(b))"},{"path":"/articles/RefDeduR.html","id":"text-cleaning-and-normalization","dir":"Articles","previous_headings":"","what":"Text cleaning and normalization","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"deduplication, first apply multiple finely-tuned text cleaning normalization dataset. finer text normalization increases chance successful deduplication exact matching step, accuracy confidence assured. step includes standard text normalization converting letters lowercase, also tailored operations response patterns observed, removing trademark “(TM)” title, removing English stop words journal, removing publisher/citation information abstract. Additionally, extract helper columns use downstream. See details norm_ extract_ functions’ documentation pages.","code":"b <- norm_df(b) # This function `norm_df` wraps all (1) text normalization and (2) helper field extraction that are needed. # By default, expect the function to add 8 more columns compared with the original data frame. # Alternatively, if you want to customize the normalization operations, refer to its sub-functions by `?norm_df` or hack the source code."},{"path":"/articles/RefDeduR.html","id":"deduplicate-by-exact-matching","dir":"Articles","previous_headings":"","what":"Deduplicate by exact matching","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"suggest first deduplicating exact matching based 1) “doi_norm”, 2) “title”, 3) “title_norm” order. DOI decisive (.e., unique publication). Title also highly selective. Note assume different research papers wouldn’t 100% identical titles text normalization. assumption hold normal cases indicated previously (1) (2). assumption may apply special publication types studies heavily focus clinical therapies. example, observed identical titles present deduplicated dataset paper.","code":""},{"path":"/articles/RefDeduR.html","id":"first-we-deduplicate-based-on-doi_norm-and-title-","dir":"Articles","previous_headings":"Deduplicate by exact matching","what":"First, we deduplicate based on “doi_norm” and “title”.","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"","code":"# We remove the identified duplicates without manual review because this is fairly conservative. b1 <- dedu_exact(b, match_by = c(\"doi_norm\", \"title\")) # The most recent version will be retained at removal."},{"path":"/articles/RefDeduR.html","id":"then-we-deduplicate-based-on-title_norm","dir":"Articles","previous_headings":"Deduplicate by exact matching","what":"Then, we deduplicate based on “title_norm”","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"make sure don’t delete unique records, introduce double_check mechanism output duplicate sets different check_by (defaults \"first_author_last_name\") b1_manual_check manual review. Usually, number duplicate sets requires review small (e.g., case, 1 set needs reviewed). worth noting incorporating double_check mechanism extremely conservative. double checking needed, can incorporate \"title_norm\" dedu_exact(). b1_manual_check empty, nothing needs manually reviewed. Otherwise, can either (1) review data frame format (e.g., preview R write.xlsx()) (2) call revtools shiny app review. Note: seems revtools::screen_duplicates() can display duplicate pairs correctly (.e., displays 2 records duplicate set 2 records). recommend trying option (1) first. Command call revtools shiny app: revtools::screen_duplicates(.data.frame(b1_manual_check)) records reviewed duplicates, can proceed removing duplicates. Otherwise, find record unique, can mark modifying match number. Using match == 3011 first_author_last_name_norm == \"de Oliveira\" example, run b1$match[(b1$match == 3011 & b1$first_author_last_name_norm == \"de Oliveira\")] <- max(b1$match)+1 Alternatively, can use function synthesisr::override_duplicates(): b1$match <- synthesisr::override_duplicates(b1$match, 3011). Note works duplicate pairs. duplicate set 2 records, can mark final record unique. finalize match, can remove duplicates. 🗒️ Although included standard pipeline, can try performing exact matching based “abstract_norm”.","code":"c(b1, b1_manual_check) %<-% dup_find_exact(b1, match_by = \"title_norm\", double_check = TRUE, check_by = \"first_author_last_name_norm\")  # Syntax %<-% must be used in this case to have the function return 2 data frames. # In this example, no unique record is found in b1_manual_check. b2 <- b1[!duplicated(b1$match), ] # The most recent version will be retained at removal.  # In order not to interfere downstream processes, we remove the \"match\" column b2 <- select(b2, -match)"},{"path":"/articles/RefDeduR.html","id":"deduplicate-by-fuzzy-matching","dir":"Articles","previous_headings":"","what":"Deduplicate by fuzzy matching","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"remove duplicates high-confidence exact matching, now proceed fuzzy matching. Fuzzy matching made calculating string similarity based Levenshtein edit distance. Two major practical challenges making fuzzy-matching deduplication process accurate high-throughput choose sensible cutoff threshold similarity score? accelerate “manual review” step reduce burden manual screening? propose two strategies address challenges correspondingly. examine similarity distribution plots use inflection point curve sensible cutoff threshold. dataset-aware method, allows fine-tuning cutoff threshold. introduce decision tree incorporates multiple fields semi-automate “manual review” step. especially helpful large datasets, case number duplicate sets requiring manual review unfeasibly high (e.g., revtools output ~1,400 duplicate sets manual confirmation treating example dataset). improve computational efficiency, divide process 2 steps: (1) order records alphabetically according title_norm compare adjacent rows; (2) perform pairwise comparisons records within group partitioned first 2 letters first_author_last_name_norm.","code":""},{"path":"/articles/RefDeduR.html","id":"part-1-order-adjacent","dir":"Articles","previous_headings":"Deduplicate by fuzzy matching","what":"Part 1: order + adjacent","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"Firstly, calculate string similarity adjacent rows columns \"title_norm\" \"abstract_norm\". plot similarity distributions normalized title abstract choose cutoffs.  plots suggest cutoff score 0.7 0.6 title 0.3 abstract. demonstration purpose, use 0.7 0.3 . selected cutoffs passed dup_find_fuzzy_adj() locate potential duplicates. function outputs 2 data frames: (1) input data frame b2 \"match\" column added (2) data frame listing id duplicate pairs (id_dup_pair_adj). 🗒️ Note inflection point serves like number begin . result usually satisfactory, may tweak values see performance can improved. Per 2nd strategy, introduce decision tree semi-automate “manual review” step. Decisions added \"decision\" column id_dup_pair. 3 levels decisions, “duplicate”, “duplicate”, “check”. decision “duplicate”, \"match\" column df modified. ensure high accuracy, especially low false positive rate, output “check” kept decision tree. get algorithm-generated decisions, can deduplicate accordingly different scenarios.","code":"c(b2, b2_simi) %<-% simi_order_adj(b2, order_by = \"title_norm\") # Computing time estimation: ~ 47 sec for this data frame (3837 rows) on a Macbook Pro (Apple M1 Pro chip basic model, memory: 16 GB). # Distribution of similarity scores based on normalized title p_b2_ti <- plot_simi_dist(b2_simi, \"title_simi\") p_b2_ti  # show plot in the Plots tab  # Distribution of similarity scores based on normalized abstract p_b2_ab <- plot_simi_dist(b2_simi, \"abstract_simi\") p_b2_ab  # show plot in the Plots tab c(b2, id_dup_pair_adj) %<-% dup_find_fuzzy_adj(b2, b2_simi, cutoff_title = 0.7, cutoff_abstract = 0.3) c(b2, id_dup_pair_adj) %<-% decision_tree_adj(b2, id_dup_pair_adj) # For the \"duplicate\", we can just deduplicate by `dup_rm_adj()`.  b2_inter <- dup_rm_adj(b2, id_dup_pair_adj)  # For the \"check\", we call revtools shiny app to review the duplicate pairs. # In the app, we select \"Yes\" for \"Is there a variable describing duplicates in this dataset?\" and \"match\" for \"Select column containing duplicate data\". # At this step, we finish removing duplicates or keeping record pairs in the app. We click \"Not duplicates\" if the pair is not duplicated. Or we click \"Select Entry #1\" or \"Select Entry #2\" to keep one of the two. # After reviewing all potential duplicates, don't forget to click \"Save Data\" and \"Exit App\" to return the results to the R workspace. In this case, the results will be returned to variable `b3`. # See revtools tutorial for more instructions: https://revtools.net/deduplication.html b3 <- revtools::screen_duplicates(b2_inter)     # remove helper columns b3 <- select(b3, -c(id, match, matches))"},{"path":"/articles/RefDeduR.html","id":"part-2-partition-pairwise","dir":"Articles","previous_headings":"Deduplicate by fuzzy matching","what":"Part 2: partition + pairwise","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"look potential duplicates according pairwise string similarity records within partitioned group columns \"title_norm\" \"abstract_norm\". default, partition dataset first 2 letters first_author_last_name_norm. efficient another popular partitioning parameter - year - datasets skewed towards recent years. Additionally, prevalence preprints, partitioning \"year\" becomes less accurate. Nevertheless, can customize partitioning parameter preference. Following pipeline similar part 1, first calculate string similarity. partition dataset, results now stored lists (compared data frames part 1). flag potential duplicates. 🗒️ cutoff thresholds can inherited part 1. avoid -deleting unique records, suggest tightening cutoff abstract similarity 0.7 (0.6) step, opposed 0.3 part 1, risk mitigated restricted ordering (contrast exhaustive pairwise comparison). apply decision tree potential duplicates. duplicate pairs “check” decision, output data frame manual review. Similarly, can either (1) review directly data frame format (e.g., preview R write.xlsx()) (2) call revtools shiny app visulization revtools::screen_duplicates(df_check_pairwise). However, don’t resolve duplicates directly revtools shiny app. Instead, use dup_resolve_pairwise() change decisions “check” “duplicate” “duplicate” according manual review results. Afterwards, remove duplicates dup_rm_pairwise().","code":"c(ls_b3, ls_b3_simi) %<-% simi_ptn_pair(b3, partition_by = \"first_two_letters_first_author_last_name\") # Computing time estimation: ~ 23 min for this data frame (3832 rows) on a Macbook Pro (Apple M1 Pro chip basic model, memory: 16 GB). You can consider running it on a high performance computing cluster if shortening the running time is of high priority. id_dup_pair_pairwise <- dup_find_fuzzy_pairwise(ls_b3, ls_b3_simi, cutoff_title = 0.7, cutoff_abstract = 0.7) id_dup_pair_pairwise <- decision_tree_pairwise(ls_b3, id_dup_pair_pairwise) df_check_pairwise <- dup_screen_pairwise(ls_b3, id_dup_pair_pairwise) # All 4 duplicate pairs in this example dataset are \"not duplicate\". id_dup_pair_pairwise <- dup_resolve_pairwise(   id_dup_pair_pairwise,   df_check_pairwise,   match_index = c(1, 2, 3, 4),   result = \"not duplicate\") b4 <- dup_rm_pairwise(ls_b3, id_dup_pair_pairwise, to_dataframe = TRUE)  # remove helper columns b4 <- select(b4, -id, -partition)"},{"path":"/articles/RefDeduR.html","id":"export-the-deduplicated-dataset-to--bib-or--ris-formats","dir":"Articles","previous_headings":"","what":"Export the deduplicated dataset to .bib or .ris formats","title":"RefDeduR: a text-normalization and decision-tree aided R package enabling accurate and high-throughput reference deduplication for large datasets","text":"Alternative function: synthesisr::write_refs(). According observation, revtools::write_bibliography() seems preserve fields exported file, ’s always good idea test functions dataset. 🗒️ export .ris file downstream applications (e.g., tested Covidence , Rayyan, Endnote) pipeline seem easier recognize .ris file .bib file.","code":"revtools::write_bibliography(b4, \"inst/extdata/dataset_deduplicated.ris\", format = \"ris\")  # export .ris # or export .bib #// revtools::write_bibliography(b4, \"inst/extdata/dataset_deduplicated.bib\", format = \"bib\")"},{"path":"/articles/RefDeduR_tutorial.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"RefDeduR tutorial","text":"scientific literature grows exponentially research becomes increasingly interdisciplinary, accurate high-throughput reference deduplication vital evidence synthesis studies (e.g., systematic reviews, meta-analyses) ensure completeness datasets reduce manual screening burden. address emerging needs, developed RefDeduR. modularize deduplication pipeline finely-tuned text normalization, three-step exact matching, two-step fuzzy matching processes. package features decision-tree algorithm considers preprints conference proceedings co-exist peer-reviewed version. , demonstrate functionality RefDeduR example pipeline.","code":""},{"path":"/articles/RefDeduR_tutorial.html","id":"example-dataset","dir":"Articles","previous_headings":"","what":"Example dataset","title":"RefDeduR tutorial","text":"use example dataset demonstrate recommended pipeline RefDeduR. dataset contains bibliographic records (n = 6384) retrieved systematic review indoor surface microbiome studies. systematic search conducted 2022-01-10 3 platforms (.e., PubMed, Web Science, Scopus).","code":""},{"path":"/articles/RefDeduR_tutorial.html","id":"pre-processing-transliterate-non-ascii-characters","dir":"Articles","previous_headings":"","what":"Pre-processing: transliterate non-ASCII characters","title":"RefDeduR tutorial","text":"transliteration process includes 2 parts: (1) transliterate common Greek letters names (e.g., α alpha, β beta) (2) transliterate accented characters ASCII characters (e.g., á , ä ). Rationale: increases chance successful deduplication exact matching. also reduces noises partitioning dataset first 2 letters first_author_last_name_norm fuzzy matching step. example, record titled “Carriage population genetics extended spectrum β-lactamase-producing Escherichia coli cats dogs New Zealand” sometimes title “Carriage population genetics extended spectrum beta-lactamase-producing Escherichia coli cats dogs New Zealand”. Author names “Álvarez-Fraga, L. Pérez, .” sometimes “Alvarez-Fraga, L. Perez, .”. ⚒️ Alternatively, python scripts developed basis unidecode package provided. scripts ready run terminal. Performance R function python scripts generally similar, little difference induced difference R package stringi python package unidecode.","code":"# Get the path to the example dataset input_file <- system.file(\"extdata\", \"dataset_raw.bib\", package = \"RefDeduR\")  # Specify the path to the output file. Here we put it in the same directory but you can modify the path to wherever you want to store the output file. transliterated_file <- system.file(\"extdata\", \"dataset_transliterated.bib\", package = \"RefDeduR\")  norm_transliteration(input_file, transliterated_file, method = c(\"greek_letter-name\", \"any-ascii\")) # Transliterate common Greek letters to their names python transliteration_greek_to_name.py <path/to/input_file> <path/to/output_file>  # Transliterate accented characters to ASCII characters  python transliteration_unaccent.py <path/to/input_file> <path/to/output_file>"},{"path":"/articles/RefDeduR_tutorial.html","id":"read-the-bibliographic-file-into-a-data-frame","dir":"Articles","previous_headings":"","what":"Read the bibliographic file into a data frame","title":"RefDeduR tutorial","text":"use function revtools::read_bibliography() read transliterated bibTex file data frame. recommend using bibTex files . According past experience, reading .ris file seems result formatting errors. Alternative function: synthesisr::read_refs() 🗒️ Comparison two import functions: synthesisr::read_refs() seems better parsing special characters. “β-α-β” can retained, text becomes “Î²-Î±-Î²” using revtools::read_bibliography(). However, potential benefit revtools::read_bibliography() keeps citation key (e.g., “RN13774” first row record “@article{RN13774,”) column named “label”. .bib file exported Endnote (case example dataset), citation key can serve unique identifier. information also preserved Covidence export. Covidence online systematic review management platform, typical downstream step following reference deduplication. preserving citation key (unique identifier) across processes desired, consider switching revtools::read_bibliography() importing twice functions combine data frames. use revtools::read_bibliography() transliterated Greek letters want unique identifier.","code":"# Read the transliterated bibTex file into a data frame b <- revtools::read_bibliography(transliterated_file)  # 6384 rows  # We can check the number of missing values in each column. # Pay attention to `title` column as we expect all records to have titles. # If your dataset has only a few NAs in title, maybe it is worth resolving the missing values manually. If your dataset has a substantial number of NAs in title (according to our experience, this is extremely rare), consider sub-setting the dataset and deduplicating separately. colSums(is.na(b))"},{"path":"/articles/RefDeduR_tutorial.html","id":"text-cleaning-and-normalization","dir":"Articles","previous_headings":"","what":"Text cleaning and normalization","title":"RefDeduR tutorial","text":"deduplication, first apply multiple finely-tuned text cleaning normalization dataset. finer text normalization increases chance successful deduplication exact matching step, accuracy confidence assured. step includes standard text normalization converting letters lowercase, also tailored operations response patterns observed, removing trademark “(TM)” title, removing English stop words journal, removing publisher/citation information abstract. Additionally, extract helper columns use downstream. See details norm_ extract_ functions’ documentation pages.","code":"b <- norm_df(b) # This function `norm_df` wraps all (1) text normalization and (2) helper field extraction that are needed. # By default, expect the function to add 8 more columns compared with the original data frame. # Alternatively, if you want to customize the normalization operations, refer to its sub-functions by `?norm_df` or hack the source code."},{"path":"/articles/RefDeduR_tutorial.html","id":"deduplicate-by-exact-matching","dir":"Articles","previous_headings":"","what":"Deduplicate by exact matching","title":"RefDeduR tutorial","text":"suggest first deduplicating exact matching based 1) “doi_norm”, 2) “title”, 3) “title_norm” order. DOI decisive (.e., unique publication). Title also highly selective. Note assume different research papers wouldn’t 100% identical titles text normalization. assumption hold normal cases indicated previously (1) (2). assumption may apply special publication types studies heavily focus clinical therapies. example, observed identical titles present deduplicated dataset paper.","code":""},{"path":"/articles/RefDeduR_tutorial.html","id":"first-we-deduplicate-based-on-doi_norm-and-title-","dir":"Articles","previous_headings":"Deduplicate by exact matching","what":"First, we deduplicate based on “doi_norm” and “title”.","title":"RefDeduR tutorial","text":"","code":"# We remove the identified duplicates without manual review because this is fairly conservative. b1 <- dedu_exact(b, match_by = c(\"doi_norm\", \"title\")) # The most recent version will be retained at removal."},{"path":"/articles/RefDeduR_tutorial.html","id":"then-we-deduplicate-based-on-title_norm","dir":"Articles","previous_headings":"Deduplicate by exact matching","what":"Then, we deduplicate based on “title_norm”","title":"RefDeduR tutorial","text":"make sure don’t delete unique records, introduce double_check mechanism output duplicate sets different check_by (defaults \"first_author_last_name\") b1_manual_check manual review. Usually, number duplicate sets requires review small (e.g., case, 1 set needs reviewed). worth noting incorporating double_check mechanism extremely conservative. double checking needed, can incorporate \"title_norm\" dedu_exact(). b1_manual_check empty, nothing needs manually reviewed. Otherwise, can either (1) review data frame format (e.g., preview R write.xlsx()) (2) call revtools shiny app review. Note: seems revtools::screen_duplicates() can display duplicate pairs correctly (.e., displays 2 records duplicate set 2 records). recommend trying option (1) first. Command call revtools shiny app: revtools::screen_duplicates(.data.frame(b1_manual_check)) records reviewed duplicates, can proceed removing duplicates. Otherwise, find record unique, can mark modifying match number. Using match == 3011 first_author_last_name_norm == \"de Oliveira\" example, run b1$match[(b1$match == 3011 & b1$first_author_last_name_norm == \"de Oliveira\")] <- max(b1$match)+1 Alternatively, can use function synthesisr::override_duplicates(): b1$match <- synthesisr::override_duplicates(b1$match, 3011). Note works duplicate pairs. duplicate set 2 records, can mark final record unique. finalize match, can remove duplicates. 🗒️ Although included standard pipeline, can try performing exact matching based “abstract_norm”.","code":"c(b1, b1_manual_check) %<-% dup_find_exact(b1, match_by = \"title_norm\", double_check = TRUE, check_by = \"first_author_last_name_norm\")  # Syntax %<-% must be used in this case to have the function return 2 data frames. # In this example, no unique record is found in b1_manual_check. b2 <- b1[!duplicated(b1$match), ] # The most recent version will be retained at removal.  # In order not to interfere downstream processes, we remove the \"match\" column b2 <- select(b2, -match)"},{"path":"/articles/RefDeduR_tutorial.html","id":"deduplicate-by-fuzzy-matching","dir":"Articles","previous_headings":"","what":"Deduplicate by fuzzy matching","title":"RefDeduR tutorial","text":"remove duplicates high-confidence exact matching, now proceed fuzzy matching. Fuzzy matching made calculating string similarity based Levenshtein edit distance. Two major practical challenges making fuzzy-matching deduplication process accurate high-throughput choose sensible cutoff threshold similarity score? accelerate “manual review” step reduce burden manual screening? propose two strategies address challenges correspondingly. examine similarity distribution plots use inflection point curve sensible cutoff threshold. dataset-aware method, allows fine-tuning cutoff threshold. introduce decision tree incorporates multiple fields semi-automate “manual review” step. especially helpful large datasets, case number duplicate sets requiring manual review unfeasibly high (e.g., revtools output ~1,400 duplicate sets manual confirmation treating example dataset). improve computational efficiency, divide process 2 steps: (1) order records alphabetically according title_norm compare adjacent rows; (2) perform pairwise comparisons records within group partitioned first 2 letters first_author_last_name_norm.","code":""},{"path":"/articles/RefDeduR_tutorial.html","id":"part-1-order-adjacent","dir":"Articles","previous_headings":"Deduplicate by fuzzy matching","what":"Part 1: order + adjacent","title":"RefDeduR tutorial","text":"Firstly, calculate string similarity adjacent rows columns \"title_norm\" \"abstract_norm\". plot similarity distributions normalized title abstract choose cutoffs.  plots suggest cutoff score 0.7 0.6 title 0.3 abstract. demonstration purpose, use 0.7 0.3 . selected cutoffs passed dup_find_fuzzy_adj() locate potential duplicates. function outputs 2 data frames: (1) input data frame b2 \"match\" column added (2) data frame listing id duplicate pairs (id_dup_pair_adj). 🗒️ Note inflection point serves like number begin . result usually satisfactory, may tweak values see performance can improved. Per 2nd strategy, introduce decision tree semi-automate “manual review” step. Decisions added \"decision\" column id_dup_pair. 3 levels decisions, “duplicate”, “duplicate”, “check”. decision “duplicate”, \"match\" column df modified. ensure high accuracy, especially low false positive rate, output “check” kept decision tree. get algorithm-generated decisions, can deduplicate accordingly different scenarios.","code":"c(b2, b2_simi) %<-% simi_order_adj(b2, order_by = \"title_norm\") # Computing time estimation: ~ 47 sec for this data frame (3837 rows) on a Macbook Pro (Apple M1 Pro chip basic model, memory: 16 GB). # Distribution of similarity scores based on normalized title p_b2_ti <- plot_simi_dist(b2_simi, \"title_simi\") p_b2_ti  # show plot in the Plots tab  # Distribution of similarity scores based on normalized abstract p_b2_ab <- plot_simi_dist(b2_simi, \"abstract_simi\") p_b2_ab  # show plot in the Plots tab c(b2, id_dup_pair_adj) %<-% dup_find_fuzzy_adj(b2, b2_simi, cutoff_title = 0.7, cutoff_abstract = 0.3) c(b2, id_dup_pair_adj) %<-% decision_tree_adj(b2, id_dup_pair_adj) # For the \"duplicate\", we can just deduplicate by `dup_rm_adj()`.  b2_inter <- dup_rm_adj(b2, id_dup_pair_adj)  # For the \"check\", we call revtools shiny app to review the duplicate pairs. # In the app, we select \"Yes\" for \"Is there a variable describing duplicates in this dataset?\" and \"match\" for \"Select column containing duplicate data\". # At this step, we finish removing duplicates or keeping record pairs in the app. We click \"Not duplicates\" if the pair is not duplicated. Or we click \"Select Entry #1\" or \"Select Entry #2\" to keep one of the two. # After reviewing all potential duplicates, don't forget to click \"Save Data\" and \"Exit App\" to return the results to the R workspace. In this case, the results will be returned to variable `b3`. # See revtools tutorial for more instructions: https://revtools.net/deduplication.html b3 <- revtools::screen_duplicates(b2_inter)     # remove helper columns b3 <- select(b3, -c(id, match, matches))"},{"path":"/articles/RefDeduR_tutorial.html","id":"part-2-partition-pairwise","dir":"Articles","previous_headings":"Deduplicate by fuzzy matching","what":"Part 2: partition + pairwise","title":"RefDeduR tutorial","text":"look potential duplicates according pairwise string similarity records within partitioned group columns \"title_norm\" \"abstract_norm\". default, partition dataset first 2 letters first_author_last_name_norm. efficient another popular partitioning parameter - year - datasets skewed towards recent years. Additionally, prevalence preprints, partitioning \"year\" becomes less accurate. Nevertheless, can customize partitioning parameter preference. Following pipeline similar part 1, first calculate string similarity. partition dataset, results now stored lists (compared data frames part 1). flag potential duplicates. 🗒️ cutoff thresholds can inherited part 1. avoid -deleting unique records, suggest tightening cutoff abstract similarity 0.7 (0.6) step, opposed 0.3 part 1, risk mitigated restricted ordering (contrast exhaustive pairwise comparison). apply decision tree potential duplicates. duplicate pairs “check” decision, output data frame manual review. Similarly, can either (1) review directly data frame format (e.g., preview R write.xlsx()) (2) call revtools shiny app visulization revtools::screen_duplicates(df_check_pairwise). However, don’t resolve duplicates directly revtools shiny app. Instead, use dup_resolve_pairwise() change decisions “check” “duplicate” “duplicate” according manual review results. Afterwards, remove duplicates dup_rm_pairwise().","code":"c(ls_b3, ls_b3_simi) %<-% simi_ptn_pair(b3, partition_by = \"first_two_letters_first_author_last_name\") # Computing time estimation: ~ 23 min for this data frame (3832 rows) on a Macbook Pro (Apple M1 Pro chip basic model, memory: 16 GB). You can consider running it on a high performance computing cluster if shortening the running time is of high priority. id_dup_pair_pairwise <- dup_find_fuzzy_pairwise(ls_b3, ls_b3_simi, cutoff_title = 0.7, cutoff_abstract = 0.7) id_dup_pair_pairwise <- decision_tree_pairwise(ls_b3, id_dup_pair_pairwise) df_check_pairwise <- dup_screen_pairwise(ls_b3, id_dup_pair_pairwise) # All 4 duplicate pairs in this example dataset are \"not duplicate\". id_dup_pair_pairwise <- dup_resolve_pairwise(   id_dup_pair_pairwise,   df_check_pairwise,   match_index = c(1, 2, 3, 4),   result = \"not duplicate\") b4 <- dup_rm_pairwise(ls_b3, id_dup_pair_pairwise, to_dataframe = TRUE)  # remove helper columns b4 <- select(b4, -id, -partition)"},{"path":"/articles/RefDeduR_tutorial.html","id":"export-the-deduplicated-dataset-to--bib-or--ris-formats","dir":"Articles","previous_headings":"","what":"Export the deduplicated dataset to .bib or .ris formats","title":"RefDeduR tutorial","text":"Alternative function: synthesisr::write_refs(). According observation, revtools::write_bibliography() seems preserve fields exported file, ’s always good idea test functions dataset. 🗒️ export .ris file downstream applications (e.g., tested Covidence , Rayyan, Endnote) pipeline seem easier recognize .ris file .bib file.","code":"revtools::write_bibliography(b4, \"inst/extdata/dataset_deduplicated.ris\", format = \"ris\")  # export .ris # or export .bib #// revtools::write_bibliography(b4, \"inst/extdata/dataset_deduplicated.bib\", format = \"bib\")"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jiaxian Shen. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Shen J (2022). RefDeduR: Package (One Line, Title Case). R package version 0.1.0.","code":"@Manual{,   title = {RefDeduR: What the Package Does (One Line, Title Case)},   author = {Jiaxian Shen},   year = {2022},   note = {R package version 0.1.0}, }"},{"path":"/index.html","id":"refdedur-","dir":"","previous_headings":"","what":"What the Package Does (One Line, Title Case)","title":"What the Package Does (One Line, Title Case)","text":"RefDeduR R package supports accurate high-throughput reference deduplication. especially useful large datasets operates standard bibliographic information (.e., require information retrieved mainstream search engine PMID). deduplication pipeline modularized finely-tuned text normalization, three-step exact matching, two-step fuzzy matching processes. package features decision-tree algorithm considers preprints conference proceedings co-exist peer-reviewed version.","code":""},{"path":"/index.html","id":"author","dir":"","previous_headings":"","what":"Author","title":"What the Package Does (One Line, Title Case)","text":"Jiaxian Shen Department Civil Environmental Engineering, Northwestern University","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"What the Package Does (One Line, Title Case)","text":"can install RefDeduR GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"jxshen311/RefDeduR\")"},{"path":"/index.html","id":"tutorial-website-and-publication","dir":"","previous_headings":"","what":"Tutorial, website and publication","title":"What the Package Does (One Line, Title Case)","text":"step--step tutorial example dataset, see <\//TODO: link vignette> complete introduction, check website. information, check publication.","code":""},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"What the Package Does (One Line, Title Case)","text":"use RefDeduR, please cite: <\//TODO: paper>","code":""},{"path":"/index.html","id":"acknowledgement","dir":"","previous_headings":"","what":"Acknowledgement","title":"What the Package Does (One Line, Title Case)","text":"thank Yutong Wu illuminating discussions design RefDeduR. also grateful Ruochen Jiao Alexander G. McFarland help coding. thank Ahmad Roaayala, Eko Purnomo, Vectors Point Noun Project allowing us use following icons Research Paper, Report Paper, report, Stats Report create logo.","code":""},{"path":"/reference/RefDeduR_example_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample data frames containing bibliographic information — RefDeduR_example_dataset","title":"Sample data frames containing bibliographic information — RefDeduR_example_dataset","text":"Data frames containing bibliographic information imported BibTeX files (file.bib) revtools::read_bibliography()","code":""},{"path":"/reference/RefDeduR_example_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample data frames containing bibliographic information — RefDeduR_example_dataset","text":"","code":"bib_example_complete  bib_example_small"},{"path":"/reference/RefDeduR_example_dataset.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample data frames containing bibliographic information — RefDeduR_example_dataset","text":"Data frames containing bibliographic information variables title, author, journal, volume, abstract. Data type columns character. bib_example_complete contains 6384 rows. bib_example_small subset bib_example_complete contains 12 rows.","code":""},{"path":"/reference/RefDeduR_example_dataset.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample data frames containing bibliographic information — RefDeduR_example_dataset","text":"bib_example_complete contains bibliographic records searching indoor surface microbiome studies PubMed, Web Science, Scopus 2022-01-10.","code":""},{"path":"/reference/decision_tree_adj.html","id":null,"dir":"Reference","previous_headings":"","what":"Make decisions for potential duplicates — decision_tree_adj","title":"Make decisions for potential duplicates — decision_tree_adj","text":"Decisions made decision tree potential duplicates identified dup_find_fuzzy_adj(). Decisions added \"decision\" column id_dup_pair. 3 levels decisions, \"duplicate\", \"duplicate\", \"check\". decision \"duplicate\", \"match\" column df  modified.","code":""},{"path":"/reference/decision_tree_adj.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make decisions for potential duplicates — decision_tree_adj","text":"","code":"decision_tree_adj(df, id_dup_pair)"},{"path":"/reference/decision_tree_adj.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make decisions for potential duplicates — decision_tree_adj","text":"df data frame (.e., output #1 dup_find_fuzzy_adj()) id_dup_pair data frame listing id potential duplicate pairs (.e., output #2 dup_find_fuzzy_adj())","code":""},{"path":"/reference/decision_tree_adj.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make decisions for potential duplicates — decision_tree_adj","text":"Two data frames: (1) input df \"match\" column modified according decision tree; (2) input id_dup_pair \"decision\" column added.","code":""},{"path":"/reference/decision_tree_adj.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make decisions for potential duplicates — decision_tree_adj","text":"See manuscript //TODO details decision tree.","code":""},{"path":"/reference/decision_tree_adj.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make decisions for potential duplicates — decision_tree_adj","text":"","code":"if (FALSE) { c(df, id_dup_pair) %<-% decision_tree_adj(df, id_dup_pair) }"},{"path":"/reference/decision_tree_pairwise.html","id":null,"dir":"Reference","previous_headings":"","what":"Make decisions for potential duplicates — decision_tree_pairwise","title":"Make decisions for potential duplicates — decision_tree_pairwise","text":"Make decisions potential duplicates","code":""},{"path":"/reference/decision_tree_pairwise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make decisions for potential duplicates — decision_tree_pairwise","text":"","code":"decision_tree_pairwise(ls_df, id_dup_pair)"},{"path":"/reference/decision_tree_pairwise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make decisions for potential duplicates — decision_tree_pairwise","text":"ls_df list data frames containing partitioned dataset  (.e., output #1 simi_ptn_pair()). id_dup_pair data frame listing record id partition id duplicate pairs (.e., output dup_find_fuzzy_pairwise()).","code":""},{"path":"/reference/decision_tree_pairwise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make decisions for potential duplicates — decision_tree_pairwise","text":"input id_dup_pair \"decision\" column added.","code":""},{"path":"/reference/decision_tree_pairwise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make decisions for potential duplicates — decision_tree_pairwise","text":"","code":"if (FALSE) { id_dup_pair <- decision_tree_pairwise(ls_df, id_dup_pair) }"},{"path":"/reference/dedu_exact.html","id":null,"dir":"Reference","previous_headings":"","what":"Find duplicates by exact match and remove them — dedu_exact","title":"Find duplicates by exact match and remove them — dedu_exact","text":"automatically removes duplicates identified exact match without manual review. recent version retained removal. Support deduplication based multiple columns (one time).","code":""},{"path":"/reference/dedu_exact.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find duplicates by exact match and remove them — dedu_exact","text":"","code":"dedu_exact(df, match_by)"},{"path":"/reference/dedu_exact.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find duplicates by exact match and remove them — dedu_exact","text":"df data frame bibliographic information gone text normalization. df must following 6 columns c(\"author\", \"title\", \"journal\", \"abstract\", \"year\", \"doi\"). match_by Quoted name(s) column(s) information match (e.g., \"doi_norm\", \"title\", \"title_norm\", c(\"doi_norm\", \"title\", \"title_norm\")). supplying character vector multiple elements, deduplication performed order. example, match_by = c(\"doi_norm\", \"title\", \"title_norm\"), deduplication performed first according \"doi_norm\", \"title\", finally \"title_norm\".","code":""},{"path":"/reference/dedu_exact.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find duplicates by exact match and remove them — dedu_exact","text":"Deduplicated df.","code":""},{"path":"/reference/dedu_exact.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find duplicates by exact match and remove them — dedu_exact","text":"Records missing information (.e., NA) match_by column modified.","code":""},{"path":"/reference/dedu_exact.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find duplicates by exact match and remove them — dedu_exact","text":"","code":"# load example dataset data(bib_example_small)  # text normalization of the data frame df <- norm_df(bib_example_small)  # deduplicate according to 3 columns in order (one at a time) df_new <- dedu_exact(df, match_by = c(\"doi_norm\", \"title\", \"title_norm\")) #> Error in df_no %>% relocate(match, .after = last_col()): could not find function \"%>%\""},{"path":"/reference/dup_find_exact.html","id":null,"dir":"Reference","previous_headings":"","what":"Find duplicates by exact match — dup_find_exact","title":"Find duplicates by exact match — dup_find_exact","text":"Find duplicates exact match","code":""},{"path":"/reference/dup_find_exact.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find duplicates by exact match — dup_find_exact","text":"","code":"dup_find_exact(   df,   match_by,   double_check = FALSE,   check_by = \"first_author_last_name_norm\" )"},{"path":"/reference/dup_find_exact.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find duplicates by exact match — dup_find_exact","text":"df data frame bibliographic information gone text normalization. df must following 6 columns c(\"author\", \"title\", \"journal\", \"abstract\", \"year\", \"doi\"). match_by Quoted name column information match (e.g., \"doi_norm\", \"title\", \"title_norm\"). double_check Logical: confirmarion another column needed? Defaults FALSE. check_by Quoted name column information double check . required/ignored double_check == FALSE. Defaults \"first_author_last_name_norm\" double_check == TRUE.","code":""},{"path":"/reference/dup_find_exact.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find duplicates by exact match — dup_find_exact","text":"double_check == FALSE, return input df new column named \"match\". double_check == TRUE, return 2 data frames (input df df_manual_check). Syntax %<-% must used case function return 2 data frames.","code":""},{"path":"/reference/dup_find_exact.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find duplicates by exact match — dup_find_exact","text":"Records missing information (.e., NA) match_by column modified. Double check criteria: Within identified duplicate set, check_by , remain duplicates without review; check_by different, output duplicate set df_manual_check manual review.","code":""},{"path":"/reference/dup_find_exact.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find duplicates by exact match — dup_find_exact","text":"","code":"library(zeallot)  # to use `%<-%`  # load example dataset data(bib_example_small)  # text normalization of the data frame df <- norm_df(bib_example_small)  # example 1: df_1 <- dup_find_exact(df, match_by = \"doi_norm\", double_check = FALSE) #> Error in df_no %>% relocate(match, .after = last_col()): could not find function \"%>%\" # Alternatively, %<-% will also work df_2 %<-% dup_find_exact(df, match_by = \"doi_norm\", double_check = FALSE) #> Error in df_no %>% relocate(match, .after = last_col()): could not find function \"%>%\"  # example 2: c(df, df_manual_check) %<-% dup_find_exact(df, match_by = \"title_norm\", double_check = TRUE, check_by = \"first_author_last_name_norm\")  # Syntax %<-% must be used in this case to have the function return 2 data frames. #> Error in df_no %>% relocate(match, .after = last_col()): could not find function \"%>%\""},{"path":"/reference/dup_find_fuzzy_adj.html","id":null,"dir":"Reference","previous_headings":"","what":"Find duplicates by fuzzy match of string similarity between adjacent rows — dup_find_fuzzy_adj","title":"Find duplicates by fuzzy match of string similarity between adjacent rows — dup_find_fuzzy_adj","text":"Find duplicates fuzzy match string similarity adjacent rows","code":""},{"path":"/reference/dup_find_fuzzy_adj.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find duplicates by fuzzy match of string similarity between adjacent rows — dup_find_fuzzy_adj","text":"","code":"dup_find_fuzzy_adj(df, df_simi, cutoff_title = 0.7, cutoff_abstract = 0.7)"},{"path":"/reference/dup_find_fuzzy_adj.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find duplicates by fuzzy match of string similarity between adjacent rows — dup_find_fuzzy_adj","text":"df ordered data frame (.e., output #1 simi_order_adj()). df_simi data frame string similarity results calculated df (.e., output #2 simi_order_adj()). cutoff_title Numeric: cutoff threshold string similarity normalized title. Range: [0, 1]. Defaults 0.7. cutoff_abstract Numeric: cutoff threshold string similarity normalized abstract. Range: [0, 1]. Defaults 0.7.","code":""},{"path":"/reference/dup_find_fuzzy_adj.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find duplicates by fuzzy match of string similarity between adjacent rows — dup_find_fuzzy_adj","text":"Two data frames: (1) input df \"match\" column added; (2) data frame listing id duplicate pairs.","code":""},{"path":"/reference/dup_find_fuzzy_adj.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find duplicates by fuzzy match of string similarity between adjacent rows — dup_find_fuzzy_adj","text":"cutoffs: recommend choosing sensible dataset-aware values according similarity distribution plot generated plot_simi_dist().","code":""},{"path":"/reference/dup_find_fuzzy_adj.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find duplicates by fuzzy match of string similarity between adjacent rows — dup_find_fuzzy_adj","text":"","code":"if (FALSE) { c(df, id_dup_pair) %<-% dup_find_fuzzy_adj(df, df_simi, cutoff_title = 0.7, cutoff_abstract = 0.7) }"},{"path":"/reference/dup_find_fuzzy_pairwise.html","id":null,"dir":"Reference","previous_headings":"","what":"Find duplicates by fuzzy match of string similarity between pairwise records — dup_find_fuzzy_pairwise","title":"Find duplicates by fuzzy match of string similarity between pairwise records — dup_find_fuzzy_pairwise","text":"Find duplicates fuzzy match string similarity pairwise records","code":""},{"path":"/reference/dup_find_fuzzy_pairwise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find duplicates by fuzzy match of string similarity between pairwise records — dup_find_fuzzy_pairwise","text":"","code":"dup_find_fuzzy_pairwise(   ls_df,   ls_df_simi,   cutoff_title = 0.7,   cutoff_abstract = 0.7 )"},{"path":"/reference/dup_find_fuzzy_pairwise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find duplicates by fuzzy match of string similarity between pairwise records — dup_find_fuzzy_pairwise","text":"ls_df list data frames containing partitioned dataset  (.e., output #1 simi_ptn_pair()). ls_df_simi list data frames string similarity results calculated (.e., output #2 simi_ptn_pair()). cutoff_title Numeric: cutoff threshold string similarity normalized title. Range: [0, 1]. Defaults 0.7. cutoff_abstract Numeric: cutoff threshold string similarity normalized abstract. Range: [0, 1]. Defaults 0.7.","code":""},{"path":"/reference/dup_find_fuzzy_pairwise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find duplicates by fuzzy match of string similarity between pairwise records — dup_find_fuzzy_pairwise","text":"data frame listing record id partition id duplicate pairs.","code":""},{"path":"/reference/dup_find_fuzzy_pairwise.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find duplicates by fuzzy match of string similarity between pairwise records — dup_find_fuzzy_pairwise","text":"cutoffs: Cutoff thresholds dup_find_fuzzy_adj() usually applicable . Alternatively, can re-examine similarity distribution plots plot_simi_dist() choose sensible values.","code":""},{"path":"/reference/dup_find_fuzzy_pairwise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find duplicates by fuzzy match of string similarity between pairwise records — dup_find_fuzzy_pairwise","text":"","code":"if (FALSE) { id_dup_pair <- dup_find_fuzzy_pairwise(ls_df, ls_df_simi, cutoff_title = 0.7, cutoff_abstract = 0.7) }"},{"path":"/reference/dup_resolve_pairwise.html","id":null,"dir":"Reference","previous_headings":"","what":"Manually resolve potential duplicate pairs requiring ","title":"Manually resolve potential duplicate pairs requiring ","text":"Change decision \"duplicate\" \"duplicate\" according manual review results.","code":""},{"path":"/reference/dup_resolve_pairwise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Manually resolve potential duplicate pairs requiring ","text":"","code":"dup_resolve_pairwise(id_dup_pair, df_check, match_index, result)"},{"path":"/reference/dup_resolve_pairwise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Manually resolve potential duplicate pairs requiring ","text":"id_dup_pair data frame listing record id partition id duplicate pairs getting automatic decisions (.e., output decision_tree_pairwise()). df_check data frame duplicate pairs manual review (.e., output dup_screen_pairwise()). match_index Numeric: vector \"match\" numbers change decision . result Character: \"duplicate\" \"duplicate\".","code":""},{"path":"/reference/dup_resolve_pairwise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Manually resolve potential duplicate pairs requiring ","text":"data frame: input id_dup_pair \"decision\" column modified accordingly.","code":""},{"path":"/reference/dup_resolve_pairwise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Manually resolve potential duplicate pairs requiring ","text":"","code":"if (FALSE) { id_dup_pair <- dup_resolve_pairwise( id_dup_pair, df_check, match_index = c(1, 2, 3, 4), result = \"not duplicate\") }"},{"path":"/reference/dup_rm_adj.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove duplicates between adjacent rows — dup_rm_adj","title":"Remove duplicates between adjacent rows — dup_rm_adj","text":"function ensures recent record kept. peer-reviewed publication co-exists preprint conference proceeding, peer-reviewed version kept.","code":""},{"path":"/reference/dup_rm_adj.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove duplicates between adjacent rows — dup_rm_adj","text":"","code":"dup_rm_adj(df, id_dup_pair)"},{"path":"/reference/dup_rm_adj.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove duplicates between adjacent rows — dup_rm_adj","text":"df data frame (.e., output #1 decision_tree_adj()) id_dup_pair data frame listing id potential duplicate pairs (.e., output #2 decision_tree_adj())","code":""},{"path":"/reference/dup_rm_adj.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove duplicates between adjacent rows — dup_rm_adj","text":"input df duplicates removed.","code":""},{"path":"/reference/dup_rm_adj.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove duplicates between adjacent rows — dup_rm_adj","text":"","code":"if (FALSE) { df_2 <- dup_rm_adj(df, id_dup_pair) }"},{"path":"/reference/dup_rm_pairwise.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove duplicates in pairwise comparison — dup_rm_pairwise","title":"Remove duplicates in pairwise comparison — dup_rm_pairwise","text":"function ensures recent record kept. peer-reviewed publication co-exists preprint conference proceeding, peer-reviewed version kept.","code":""},{"path":"/reference/dup_rm_pairwise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove duplicates in pairwise comparison — dup_rm_pairwise","text":"","code":"dup_rm_pairwise(ls_df, id_dup_pair, to_dataframe = TRUE)"},{"path":"/reference/dup_rm_pairwise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove duplicates in pairwise comparison — dup_rm_pairwise","text":"ls_df list data frames containing partitioned dataset  (.e., output #1 simi_ptn_pair()). id_dup_pair data frame listing record id partition id duplicate pairs resolving checked duplicates (.e., output dup_resolve_pairwise()). to_dataframe Logical: merge list data frames single data frame? Defaults TRUE.","code":""},{"path":"/reference/dup_rm_pairwise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove duplicates in pairwise comparison — dup_rm_pairwise","text":"input ls_df duplicates removed. resulted list data frames merged single data frame to_dataframe == TRUE. Otherwise, list data frames returned.","code":""},{"path":"/reference/dup_rm_pairwise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove duplicates in pairwise comparison — dup_rm_pairwise","text":"","code":"if (FALSE) { df_2 <- dup_rm_pairwise(ls_df, id_dup_pair, to_dataframe = TRUE) # or ls_df_2 <- dup_rm_pairwise(ls_df, id_dup_pair, to_dataframe = FALSE) }"},{"path":"/reference/dup_screen_pairwise.html","id":null,"dir":"Reference","previous_headings":"","what":"Output potential duplicates determined as requiring manual check by the decision tree — dup_screen_pairwise","title":"Output potential duplicates determined as requiring manual check by the decision tree — dup_screen_pairwise","text":"Output potential duplicates determined requiring manual check decision tree","code":""},{"path":"/reference/dup_screen_pairwise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Output potential duplicates determined as requiring manual check by the decision tree — dup_screen_pairwise","text":"","code":"dup_screen_pairwise(ls_df, id_dup_pair)"},{"path":"/reference/dup_screen_pairwise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Output potential duplicates determined as requiring manual check by the decision tree — dup_screen_pairwise","text":"ls_df list data frames containing partitioned dataset  (.e., output #1 simi_ptn_pair()). id_dup_pair data frame listing record id partition id duplicate pairs getting automatic decisions (.e., output decision_tree_pairwise()).","code":""},{"path":"/reference/dup_screen_pairwise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Output potential duplicates determined as requiring manual check by the decision tree — dup_screen_pairwise","text":"data frame duplicate pairs manual review. Pairing indicated \"match\" column.","code":""},{"path":"/reference/dup_screen_pairwise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Output potential duplicates determined as requiring manual check by the decision tree — dup_screen_pairwise","text":"","code":"if (FALSE) { df_check <- dup_screen_pairwise(ls_df, id_dup_pair) }"},{"path":"/reference/extract_initialism.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract initialism\nExtract 1st letter of each word and delete all the other letters — extract_initialism","title":"Extract initialism\nExtract 1st letter of each word and delete all the other letters — extract_initialism","text":"Extract initialism Extract 1st letter word delete letters","code":""},{"path":"/reference/extract_initialism.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract initialism\nExtract 1st letter of each word and delete all the other letters — extract_initialism","text":"","code":"extract_initialism(string)"},{"path":"/reference/extract_initialism.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract initialism\nExtract 1st letter of each word and delete all the other letters — extract_initialism","text":"string character vector (e.g., column data frame)","code":""},{"path":"/reference/extract_initialism.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract initialism\nExtract 1st letter of each word and delete all the other letters — extract_initialism","text":"character vector","code":""},{"path":"/reference/extract_initialism.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract initialism\nExtract 1st letter of each word and delete all the other letters — extract_initialism","text":"","code":"# Example 1 journal <- c(\"JOURNAL OF BIOLOGICAL CHEMISTRY\",              \"Proteins: Structure, Function and Bioinformatics\",              \"Archives of Biochemistry and Biophysics\",              \"Clin Infect Dis\",              \"Clinical Infectious Diseases\",              \"CLINICAL INFECTIOUS DISEASES\")  extract_initialism(journal) #> [1] \"JOBC\"  \"PSFAB\" \"AOBAB\" \"CID\"   \"CID\"   \"CID\"     # Example 2 journal_norm <- c(\"journal biological chemistry\",                   \"proteins structure function bioinformatics\",                   \"archives biochemistry biophysics\",                   \"clin infect dis\",                   \"clinical infectious diseases\",                   \"clinical infectious diseases\")  extract_initialism(journal_norm) #> [1] \"JBC\"  \"PSFB\" \"ABB\"  \"CID\"  \"CID\"  \"CID\""},{"path":"/reference/norm_abstract.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean and normalize abstract in bibliography — norm_abstract","title":"Clean and normalize abstract in bibliography — norm_abstract","text":"abstract, following string normalization. Remove tailing information \". (C) 1998 International Astronautical Federation Published Elsevier Science Ltd. rights reserved.\" \"(C) 2000 Elsevier Science B.V. rights reserved.\" according 6 patterns observed empirically, reduce effect information. convert letters lowercase remove whitespace start end string; also reduce repeated whitespace inside string.","code":""},{"path":"/reference/norm_abstract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean and normalize abstract in bibliography — norm_abstract","text":"","code":"norm_abstract(abstract, first_author_last_name)"},{"path":"/reference/norm_abstract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean and normalize abstract in bibliography — norm_abstract","text":"abstract character vector (e.g., column data frame) first_author_last_name character vector containing last name first author, FALSE. default, suggest supplying first_author_last_name. FALSE, index abstract first_author_last_name must bibliographic record. issue analyzing based data frame. Last name first author used one patterns remove irrelevant information abstract clean text. example information removed includes \". (c) Daisuke Fujiwara et al., 2021;\" FALSE, abstract normalization according pattern bypassed.","code":""},{"path":"/reference/norm_abstract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean and normalize abstract in bibliography — norm_abstract","text":"Normalized character vector","code":""},{"path":"/reference/norm_abstract.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean and normalize abstract in bibliography — norm_abstract","text":"","code":"data(bib_example_small)  bib_example_small$first_author_last_name <- sapply(stringr::str_split(bib_example_small$author, \",\",  n = 2), `[`, 1)   bib_example_small$abstract_norm <- norm_abstract(bib_example_small$abstract, bib_example_small$first_author_last_name) # or bib_example_small$abstract_norm <- norm_abstract(bib_example_small$abstract, FALSE)"},{"path":"/reference/norm_author.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean and normalize author in bibliography — norm_author","title":"Clean and normalize author in bibliography — norm_author","text":"author, following string normalization. replace “” “&” (Reduce effect dissimilarity) remove space (1. Remove extra whitespace  2. Reduce effect space-caused dissimilarity) convert letters lowercase","code":""},{"path":"/reference/norm_author.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean and normalize author in bibliography — norm_author","text":"","code":"norm_author(author, rm_punctuation = FALSE)"},{"path":"/reference/norm_author.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean and normalize author in bibliography — norm_author","text":"author character vector (e.g., column data frame) rm_punctuation Logical: unaccenting characters introduce extra punctuation? , need removed. Defaults FALSE. Using bash iconv unaccent characters introduce extra punctuation (e.g., '`^~\\\"). using bash iconv, punctuation needs removed well. Since use python now, needed default.","code":""},{"path":"/reference/norm_author.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean and normalize author in bibliography — norm_author","text":"Normalized character vector","code":""},{"path":"/reference/norm_author.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean and normalize author in bibliography — norm_author","text":"","code":"# Example 1 author <- c(\"Xia, Z. X. and Dai, W. W. and Xiong, J. P. and Hao, Z. P. and Davidson, V. L. and White, S. and Mathews, F. S.\", \"Ahmed, M. H. and Koparde, V. N. and Safo, M. K. and Neel Scarsdale, J. and Kellogg, G. E.\", \"Whitman, C. P.\" )  norm_author(author) #> [1] \"xia,z.x.&dai,w.w.&xiong,j.p.&hao,z.p.&davidson,v.l.&white,s.&mathews,f.s.\" #> [2] \"ahmed,m.h.&koparde,v.n.&safo,m.k.&neelscarsdale,j.&kellogg,g.e.\"           #> [3] \"whitman,c.p.\"                                                                # Example 2 # é becomes 'e if you use `cat file.bib | iconv -f utf8 -t ascii//TRANSLIT//IGNORE > convert.bib` to unaccent characters. Make `rm_punctuation = TRUE` to remove the extra punctuation intruduced. author2 <- c(\"Ren'ee\")  norm_author(author2, rm_punctuation = TRUE) #> [1] \"renee\""},{"path":"/reference/norm_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean and normalize the entire data frame with bibliographic information — norm_df","title":"Clean and normalize the entire data frame with bibliographic information — norm_df","text":"Normally bibliographic information imported data frame rather individual character vectors. function norm_df wraps (1) text normalization (2) helper field extraction needed downstream reference deduplication.","code":""},{"path":"/reference/norm_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean and normalize the entire data frame with bibliographic information — norm_df","text":"","code":"norm_df(df)"},{"path":"/reference/norm_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean and normalize the entire data frame with bibliographic information — norm_df","text":"df data frame bibliographic information","code":""},{"path":"/reference/norm_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean and normalize the entire data frame with bibliographic information — norm_df","text":"normalized data frame. New columns containing normalized extracted data added original data frame. default, expect 8 columns compared original data frame.","code":""},{"path":"/reference/norm_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Clean and normalize the entire data frame with bibliographic information — norm_df","text":"detail, normalizes doi (simply convert lowercase; add column doi_norm) title (see sub-function norm_title(); add column title_norm) author (see sub-function norm_author(); add column author_norm) journal (see sub-function norm_journal(); add column journal_norm) year (simply convert data type character integer; add column) abstract (see sub-function norm_abstract(); add column abstract_norm) Additionally, extracts first_author_last_name first_author_last_name_norm journal_initialism (apply function extract_initialism() column journal_norm).","code":""},{"path":[]},{"path":"/reference/norm_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean and normalize the entire data frame with bibliographic information — norm_df","text":"","code":"data(bib_example_small)  df_new <- norm_df(bib_example_small)"},{"path":"/reference/norm_journal.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean and normalize journal in bibliography — norm_journal","title":"Clean and normalize journal in bibliography — norm_journal","text":"journal, following string normalization. convert letters lowercase remove punctuation remove English stop words remove whitespace start end string; also reduce repeated whitespace inside string.","code":""},{"path":"/reference/norm_journal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean and normalize journal in bibliography — norm_journal","text":"","code":"norm_journal(journal)"},{"path":"/reference/norm_journal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean and normalize journal in bibliography — norm_journal","text":"journal character vector (e.g., column data frame)","code":""},{"path":"/reference/norm_journal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean and normalize journal in bibliography — norm_journal","text":"Normalized character vector","code":""},{"path":"/reference/norm_journal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean and normalize journal in bibliography — norm_journal","text":"","code":"journal <- c(\"Proteins: Structure, Function and Bioinformatics\", \"Zoonoses Public Health\", \"Zoonoses and Public Health\")  norm_journal(journal) #> [1] \"proteins structure function bioinformatics\" #> [2] \"zoonoses public health\"                     #> [3] \"zoonoses public health\""},{"path":"/reference/norm_title.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean and normalize title in bibliography — norm_title","title":"Clean and normalize title in bibliography — norm_title","text":"title, following string normalization. remove trademark \"(TM)\" convert letters lowercase remove punctuation remove space","code":""},{"path":"/reference/norm_title.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean and normalize title in bibliography — norm_title","text":"","code":"norm_title(title)"},{"path":"/reference/norm_title.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean and normalize title in bibliography — norm_title","text":"title character vector (e.g., column data frame)","code":""},{"path":"/reference/norm_title.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean and normalize title in bibliography — norm_title","text":"Normalized character vector","code":""},{"path":"/reference/norm_title.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean and normalize title in bibliography — norm_title","text":"","code":"title <- c(\"Evaluation of the Abbott RealTime (TM) CT assay with the BD ProbeTec (TM) ET assay for the detection of Chlamydia trachomatis in a clinical microbiology laboratory\", \"Evaluation of the Abbott RealTime CT assay with the BD ProbeTec ET assay for the detection of Chlamydia trachomatis in a clinical microbiology laboratory\", \"Evaluation of the Abbott RealTime(tm) CT assay with the BD ProbeTec(tm) ET assay for the detection of Chlamydia trachomatis in a clinical microbiology laboratory\", \"beta-lactam Resistance in Pseudomonas aeruginosa: Current Status, Future Prospects\" )  norm_title(title) #> [1] \"evaluationoftheabbottrealtimectassaywiththebdprobetecetassayforthedetectionofchlamydiatrachomatisinaclinicalmicrobiologylaboratory\" #> [2] \"evaluationoftheabbottrealtimectassaywiththebdprobetecetassayforthedetectionofchlamydiatrachomatisinaclinicalmicrobiologylaboratory\" #> [3] \"evaluationoftheabbottrealtimectassaywiththebdprobetecetassayforthedetectionofchlamydiatrachomatisinaclinicalmicrobiologylaboratory\" #> [4] \"betalactamresistanceinpseudomonasaeruginosacurrentstatusfutureprospects\""},{"path":"/reference/norm_transliteration.html","id":null,"dir":"Reference","previous_headings":"","what":"Transliterate a text file — norm_transliteration","title":"Transliterate a text file — norm_transliteration","text":"Transliterate text file output another text file.","code":""},{"path":"/reference/norm_transliteration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transliterate a text file — norm_transliteration","text":"","code":"norm_transliteration(   file_input,   file_output,   method = c(\"greek_letter-name\", \"any-ascii\"),   custom )"},{"path":"/reference/norm_transliteration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transliterate a text file — norm_transliteration","text":"file_input Directory/Path input file. file_output Directory/Path file. need create file advance. file specified name created. method character vector specifying transliteration rules order. Defaults recommended rule combination - c(\"greek_letter-name\", \"-ascii\"). See stringi::stri_trans_general() documentation rules database. Custom rules also allowed (see examples). custom Logical. logical vector (length method) specifying whether corresponding transliteration rule custom rule . Optional none rules custom (.e., Defaults -FALSE vector.).","code":""},{"path":"/reference/norm_transliteration.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Transliterate a text file — norm_transliteration","text":"Transliteration rule \"greek_letter-name\" transliterates common Greek letters names (e.g., α alpha, β beta). See ls_greek_letter_to_name() complete list transformations. rules stringi::stri_trans_general() applicable rule method . example, \"-ascii\" transform scripts ASCII format. See stringi::stri_trans_general() documentation details. See Example 3 last example stringi::stri_trans_general() instrucstions construct custom rules.","code":""},{"path":"/reference/norm_transliteration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transliterate a text file — norm_transliteration","text":"","code":"# An example input file has been put in extdata/ # We use system.file() to output the path to this example file input <- system.file(\"extdata\", \"example_norm_trans.bib\", package = \"RefDeduR\") # The input file looks like this. #> author = {Whitman, C. P., Álvarez-Fraga, L. and Pérez, A á ä}, #> title = {β-α-β structural motif}, #> Ϛ  # Specify the path to the output file. Here I put it in the same directory but you can modify the path to wherever you want to store the output file. output <- system.file(\"extdata\", \"output.bib\", package = \"RefDeduR\")   # Example 1: default setting norm_transliteration(input, output, method = c(\"greek_letter-name\", \"any-ascii\")) #> Warning: file(\"\") only supports open = \"w+\" and open = \"w+b\": using the former # For demonstration purpose, output.bib will look like this. #> author = {Whitman, C. P., Alvarez-Fraga, L. and Perez, A a a}, #> title = {beta-alpha-beta structural motif}, #> Ϛ   # Example 2: use other transliteration rules in the database norm_transliteration(input, output, method = c(\"Greek-en_US/UNGEGN\", \"any-ascii\")) #> Warning: file(\"\") only supports open = \"w+\" and open = \"w+b\": using the former # For demonstration purpose, output.bib will look like this. #> author = {Whitman, C. P., Alvarez-Fraga, L. and Perez, A a a}, #> title = {b-a-b structural motif}, #> Ϛ   # Example 3: use custom transliteration rules id_custom <- \" \\u03DA > 'Stigma'; \\u03E0 > 'Sampi'; \"  # Note that every transliteration needs to be accompanied by a semicolon, including the last line.  norm_transliteration(input, output, method = c(\"greek_letter-name\", id_custom, \"any-ascii\"), custom = c(FALSE, TRUE, FALSE)) #> Warning: file(\"\") only supports open = \"w+\" and open = \"w+b\": using the former # For demonstration purpose, output.bib will look like this. #> author = {Whitman, C. P., Alvarez-Fraga, L. and Perez, A a a}, #> title = {beta-alpha-beta structural motif}, #> Stigma"},{"path":"/reference/plot_simi_dist.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate similarity distribution plot — plot_simi_dist","title":"Generate similarity distribution plot — plot_simi_dist","text":"Generate similarity distribution plot","code":""},{"path":"/reference/plot_simi_dist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate similarity distribution plot — plot_simi_dist","text":"","code":"plot_simi_dist(df_simi, simi_param)"},{"path":"/reference/plot_simi_dist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate similarity distribution plot — plot_simi_dist","text":"df_simi data frame string similarity results. simi_param character similarity parameter plot distribution (.e., quoted name column containing similarity scores). example, \"title_simi\", \"abstract_simi\", \"author_simi\", \"first_author_last_name_simi\".","code":""},{"path":"/reference/plot_simi_dist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate similarity distribution plot — plot_simi_dist","text":"scatterplot distributions similarity scores","code":""},{"path":"/reference/plot_simi_dist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate similarity distribution plot — plot_simi_dist","text":"","code":"if (FALSE) { # `df_simi` is the data frame resulted from `simi_order_df()` # see the example in the `simi_order_df()` help page for how `df_simi` is generated  # Distribution of similarity scores based on normalized title p_ti <- plot_simi_dist(df_simi, \"title_simi\") p_ti  # show p_ti in the Plots tab  # Distribution of similarity scores based on normalized abstract p_ab <- plot_simi_dist(df_simi, \"abstract_simi\") p_ab  # show p_ab in the Plots tab }"},{"path":"/reference/simi_order_adj.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate string similarity between adjacent rows — simi_order_adj","title":"Calculate string similarity between adjacent rows — simi_order_adj","text":"function calculates similarity based Levenshtein edit distance columns \"title_norm\" \"abstract_norm\" adjacent rows. Range similarity [0, 1]. Similarity == 1 means 100% identical Similarity == 0 means completely different.","code":""},{"path":"/reference/simi_order_adj.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate string similarity between adjacent rows — simi_order_adj","text":"","code":"simi_order_adj(df, order_by = \"title_norm\")"},{"path":"/reference/simi_order_adj.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate string similarity between adjacent rows — simi_order_adj","text":"df data frame bibliographic information gone text normalization. df must following columns c(\"title_norm\", \"abstract_norm\"). order_by Quoted name column order rows. Defaults \"title_norm\".","code":""},{"path":"/reference/simi_order_adj.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate string similarity between adjacent rows — simi_order_adj","text":"Two data frames: (1) Ordered df; (2) data frame string similarity results \"title_norm\" \"abstract_norm\". data frames matched id column.","code":""},{"path":"/reference/simi_order_adj.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate string similarity between adjacent rows — simi_order_adj","text":"function based assumption records titles. Computing time estimation according past experience: ~ 46 seconds data frame 3837 rows Macbook Pro (Apple M1 Pro chip basic model, memory: 16 GB).","code":""},{"path":"/reference/simi_order_adj.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate string similarity between adjacent rows — simi_order_adj","text":"","code":"if (FALSE) { # load example dataset data(bib_example_small)  # text normalization of the data frame df <- norm_df(bib_example_small)  # calculate similarity c(df, df_simi) %<-% simi_order_adj(df, order_by = \"title_norm\") # df_simi[1, ] stores similarity results between df[1, ] and df[2, ] }"},{"path":"/reference/simi_ptn_pair.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate pairwise string similarity — simi_ptn_pair","title":"Calculate pairwise string similarity — simi_ptn_pair","text":"function calculates pairwise similarity based Levenshtein edit distance columns \"title_norm\" \"abstract_norm\" records within group partitioning. Range similarity [0, 1]. Similarity == 1 means 100% identical Similarity == 0 means completely different.","code":""},{"path":"/reference/simi_ptn_pair.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate pairwise string similarity — simi_ptn_pair","text":"","code":"simi_ptn_pair(df, partition_by = \"first_two_letters_first_author_last_name\")"},{"path":"/reference/simi_ptn_pair.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate pairwise string similarity — simi_ptn_pair","text":"df data frame bibliographic information gone text normalization. df must following columns c(\"title_norm\", \"abstract_norm\"). partition_by Quoted name column partition rows. Defaults \"first_two_letters_first_author_last_name\". Can FALSE prefer partition, case records compared others. Besides default, \"year\" another popular partitioning parameter. recommend default method papers dataset evenly distributed across years. instance, papers recent, dafault method much efficient \"year\". Additionally, prevalence preprints, partitioning \"year\" becomes less accurate. addition, can also construct custom \"partition\" column.","code":""},{"path":"/reference/simi_ptn_pair.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate pairwise string similarity — simi_ptn_pair","text":"Two list data frames: (1) list data frames containing partitioned df; (2) list data frames string similarity results \"title_norm\" \"abstract_norm\".","code":""},{"path":"/reference/simi_ptn_pair.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate pairwise string similarity — simi_ptn_pair","text":"artificial code \"00\" assigned cells missing values partition_by column rows partitioned one group. customize partitioning parameter, try avoid artificial code. Computing time estimation according past experience: ~ 23 min data frame 3832 rows Macbook Pro (Apple M1 Pro chip basic model, memory: 16 GB). Consider running high performance computing cluster want shorten time.","code":""},{"path":"/reference/simi_ptn_pair.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate pairwise string similarity — simi_ptn_pair","text":"","code":"if (FALSE) { # load example dataset data(bib_example_small)  # text normalization of the data frame df <- norm_df(bib_example_small)  # calculate similarity c(ls_df, ls_df_simi) %<-% simi_ptn_pair(df, partition_by = \"first_two_letters_first_author_last_name\") }"}]
